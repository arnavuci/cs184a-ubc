{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":45867,"databundleVersionId":6924515,"sourceType":"competition"},{"sourceId":3729,"sourceType":"modelInstanceVersion","modelInstanceId":2656}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Model and Data Imports","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport os\nimport gc\nimport cv2\n# import math\nimport copy\nimport time\nimport random\nimport glob\n\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Image processing\nimport timm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n\n# Pytorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\nimport torchvision\n\n# Utils\nimport joblib\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\nsr_ = Style.RESET_ALL\n\n\n# Format and filter potential warnings and errors\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:19.281780Z","iopub.execute_input":"2023-12-05T17:45:19.282254Z","iopub.status.idle":"2023-12-05T17:45:19.291871Z","shell.execute_reply.started":"2023-12-05T17:45:19.282197Z","shell.execute_reply":"2023-12-05T17:45:19.290910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image and Model Hyperparameters","metadata":{}},{"cell_type":"code","source":"CONFIG = {\n    \"seed\": 42,\n    \"epochs\": 15,\n    \"img_size\": 512,\n    \"model_name\": \"tf_efficientnet_b0_ns\",\n    \"checkpoint_path\" : \"/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b0/1/tf_efficientnet_b0_aa-827b6e33.pth\",\n    \"num_classes\": 5,\n    \"train_batch_size\": 32,\n    \"valid_batch_size\": 64,\n    \"learning_rate\": 0.0001,\n    \"scheduler\": 'CosineAnnealingLR',\n    \"min_lr\": 0.000001,\n    \"T_max\": 500,\n    \"weight_decay\": 0.000001,\n    \"fold\" : 0,\n    \"n_fold\": 5,\n    \"n_accumulate\": 1,\n    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:19.293939Z","iopub.execute_input":"2023-12-05T17:45:19.294401Z","iopub.status.idle":"2023-12-05T17:45:19.308219Z","shell.execute_reply.started":"2023-12-05T17:45:19.294376Z","shell.execute_reply":"2023-12-05T17:45:19.307508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set Seed","metadata":{}},{"cell_type":"code","source":"def set_seed(seed=42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    \n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:19.309196Z","iopub.execute_input":"2023-12-05T17:45:19.309456Z","iopub.status.idle":"2023-12-05T17:45:19.319420Z","shell.execute_reply.started":"2023-12-05T17:45:19.309434Z","shell.execute_reply":"2023-12-05T17:45:19.318525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT_DIR = '/kaggle/input/UBC-OCEAN'\nTRAIN_DIR = '/kaggle/input/UBC-OCEAN/train_thumbnails'\nTEST_DIR = '/kaggle/input/UBC-OCEAN/test_images'","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:19.320561Z","iopub.execute_input":"2023-12-05T17:45:19.320832Z","iopub.status.idle":"2023-12-05T17:45:19.331994Z","shell.execute_reply.started":"2023-12-05T17:45:19.320809Z","shell.execute_reply":"2023-12-05T17:45:19.331151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_file_path(image_id):\n    return f\"{TRAIN_DIR}/{image_id}_thumbnail.png\"","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:19.334281Z","iopub.execute_input":"2023-12-05T17:45:19.334609Z","iopub.status.idle":"2023-12-05T17:45:19.340943Z","shell.execute_reply.started":"2023-12-05T17:45:19.334573Z","shell.execute_reply":"2023-12-05T17:45:19.340018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Retrieve Data from CSV Files and Link Them to Thumbnails","metadata":{}},{"cell_type":"code","source":"train_images = sorted(glob.glob(f\"{TRAIN_DIR}/*.png\"))","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:19.342119Z","iopub.execute_input":"2023-12-05T17:45:19.342437Z","iopub.status.idle":"2023-12-05T17:45:19.356807Z","shell.execute_reply.started":"2023-12-05T17:45:19.342401Z","shell.execute_reply":"2023-12-05T17:45:19.356007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(f\"{ROOT_DIR}/train.csv\")\ndf['file_path'] = df['image_id'].apply(get_train_file_path)\ndf = df[ df[\"file_path\"].isin(train_images) ].reset_index(drop=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:19.357862Z","iopub.execute_input":"2023-12-05T17:45:19.358123Z","iopub.status.idle":"2023-12-05T17:45:19.379226Z","shell.execute_reply.started":"2023-12-05T17:45:19.358100Z","shell.execute_reply":"2023-12-05T17:45:19.378392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counts = [0, 0, 0, 0, 0]\nlabels = ['CC', 'EC', 'HGSC', 'LGSC', 'MC']\n\nfor index, row in df.iterrows():\n    counts[labels.index(row['label'])] += 1\n\nfig, ax = plt.subplots(1, 1)\nax.set_title('Distribution of Class Labels in Training Dataset')\nax.pie(counts, labels=labels, autopct='%1.0f%%')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:19.380247Z","iopub.execute_input":"2023-12-05T17:45:19.380522Z","iopub.status.idle":"2023-12-05T17:45:19.566842Z","shell.execute_reply.started":"2023-12-05T17:45:19.380500Z","shell.execute_reply":"2023-12-05T17:45:19.565518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = LabelEncoder()\ndf['label'] = encoder.fit_transform(df['label'])\n\nwith open(\"label_encoder.pkl\", \"wb\") as fp:\n    joblib.dump(encoder, fp)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:19.568630Z","iopub.execute_input":"2023-12-05T17:45:19.569473Z","iopub.status.idle":"2023-12-05T17:45:19.580568Z","shell.execute_reply.started":"2023-12-05T17:45:19.569421Z","shell.execute_reply":"2023-12-05T17:45:19.579179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG['T_max'] = df.shape[0] * (CONFIG[\"n_fold\"]-1) * CONFIG['epochs'] // CONFIG['train_batch_size'] // CONFIG[\"n_fold\"]\nCONFIG['T_max']","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:19.582516Z","iopub.execute_input":"2023-12-05T17:45:19.583466Z","iopub.status.idle":"2023-12-05T17:45:19.594122Z","shell.execute_reply.started":"2023-12-05T17:45:19.583414Z","shell.execute_reply":"2023-12-05T17:45:19.592575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Folds for k-Fold Cross Validation","metadata":{}},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=CONFIG['n_fold'])\n\nfor fold, ( _, val_) in enumerate(skf.split(X=df, y=df.label)):\n      df.loc[val_ , \"kfold\"] = int(fold)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:19.600547Z","iopub.execute_input":"2023-12-05T17:45:19.601303Z","iopub.status.idle":"2023-12-05T17:45:19.617071Z","shell.execute_reply.started":"2023-12-05T17:45:19.601259Z","shell.execute_reply":"2023-12-05T17:45:19.615897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Format Data","metadata":{}},{"cell_type":"markdown","source":"## Data Class","metadata":{}},{"cell_type":"markdown","source":"Will be used to get relevant image information from thumbnails that are linked to csv file entries.","metadata":{}},{"cell_type":"code","source":"class UBCData(Dataset):\n    def __init__(self, df, transforms=None):\n        self.df = df\n        self.file_names = df['file_path'].values\n        self.labels = df['label'].values\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path = self.file_names[index]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        label = self.labels[index]\n        \n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n            \n        return {\n            'image': img,\n            'label': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:19.619003Z","iopub.execute_input":"2023-12-05T17:45:19.619793Z","iopub.status.idle":"2023-12-05T17:45:19.632361Z","shell.execute_reply.started":"2023-12-05T17:45:19.619749Z","shell.execute_reply":"2023-12-05T17:45:19.631123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Format and Transform Images ","metadata":{}},{"cell_type":"markdown","source":"Images need to be preprocessed before being fed into the CNN because their current size is too big to efficiently handle.","metadata":{}},{"cell_type":"code","source":"data_transforms = {\n    \"train\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.ShiftScaleRotate(shift_limit=0.1, \n                           scale_limit=0.15, \n                           rotate_limit=60, \n                           p=0.5),\n        A.HueSaturationValue(\n                hue_shift_limit=0.2, \n                sat_shift_limit=0.2, \n                val_shift_limit=0.2, \n                p=0.5\n            ),\n        A.RandomBrightnessContrast(\n                brightness_limit=(-0.1,0.1), \n                contrast_limit=(-0.1, 0.1), \n                p=0.5\n            ),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.),\n    \n    \"valid\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.)\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:19.633715Z","iopub.execute_input":"2023-12-05T17:45:19.636536Z","iopub.status.idle":"2023-12-05T17:45:19.645091Z","shell.execute_reply.started":"2023-12-05T17:45:19.636510Z","shell.execute_reply":"2023-12-05T17:45:19.644271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GeM Pooling Implemenation","metadata":{}},{"cell_type":"markdown","source":"Implementation taken from the following [link](https://amaarora.github.io/posts/2020-08-30-gempool.html)","metadata":{}},{"cell_type":"code","source":"class GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM, self).__init__()\n        self.p = nn.Parameter(torch.ones(1)*p)\n        self.eps = eps\n\n    def forward(self, x):\n        return self.gem(x, p=self.p, eps=self.eps)\n        \n    def gem(self, x, p=3, eps=1e-6):\n        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + \\\n                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n                ', ' + 'eps=' + str(self.eps) + ')'","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:19.646289Z","iopub.execute_input":"2023-12-05T17:45:19.646585Z","iopub.status.idle":"2023-12-05T17:45:19.654915Z","shell.execute_reply.started":"2023-12-05T17:45:19.646561Z","shell.execute_reply":"2023-12-05T17:45:19.653985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating a Model","metadata":{}},{"cell_type":"code","source":"class UBCModel(nn.Module):\n    def __init__(self, model_name, num_classes, pretrained=True, checkpoint_path=None):\n        super(UBCModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, checkpoint_path=checkpoint_path)\n\n        inputs = self.model.classifier.in_features\n        self.model.classifier = nn.Identity()\n        self.model.global_pool = nn.Identity()\n        self.pooling = GeM()\n#         self.pooling = nn.AdaptiveAvgPool2d((1, 1))\n#         self.pooling = nn.AdaptiveMaxPool2d((1, 1))\n        self.linear = nn.Linear(inputs, num_classes)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, images):\n        out = self.model(images)\n        out = self.pooling(out).flatten(1)\n        out = self.linear(out)\n        return out\n\n    \nmodel = UBCModel(CONFIG['model_name'], CONFIG['num_classes'], checkpoint_path=CONFIG['checkpoint_path'])\nmodel.to(CONFIG['device']);","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:19.655996Z","iopub.execute_input":"2023-12-05T17:45:19.656324Z","iopub.status.idle":"2023-12-05T17:45:19.968118Z","shell.execute_reply.started":"2023-12-05T17:45:19.656293Z","shell.execute_reply":"2023-12-05T17:45:19.967295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implement Loss Function","metadata":{}},{"cell_type":"markdown","source":"For this problem, we can use the built-in CrossEntropyLoss function as our loss function.","metadata":{}},{"cell_type":"code","source":"def criterion(outputs, labels):\n    return nn.CrossEntropyLoss()(outputs, labels)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:19.969171Z","iopub.execute_input":"2023-12-05T17:45:19.969440Z","iopub.status.idle":"2023-12-05T17:45:19.973971Z","shell.execute_reply.started":"2023-12-05T17:45:19.969417Z","shell.execute_reply":"2023-12-05T17:45:19.973089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training (for a single epoch)","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    \n    data_size = 0\n    loss = 0.0\n    accuracy  = 0.0\n    \n    # Represent training process as a progress bar\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        images = data['image'].to(device, dtype=torch.float)\n        labels = data['label'].to(device, dtype=torch.long)\n        \n        batch_size = images.size(0)\n        \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss = loss / CONFIG['n_accumulate']\n            \n        loss.backward()\n    \n        if (step + 1) % CONFIG['n_accumulate'] == 0:\n            optimizer.step()\n\n            optimizer.zero_grad()\n            \n            # Increment the scheduler\n            if scheduler is not None:\n                scheduler.step()\n                \n        _, predicted = torch.max(model.softmax(outputs), 1)\n        acc = torch.sum( predicted == labels )\n        \n        loss += (loss.item() * batch_size)\n        accuracy  += acc.item()\n        data_size += batch_size\n        \n        epoch_loss = loss / data_size\n        epoch_acc = accuracy / data_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss, Train_Acc=epoch_acc,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    \n    return epoch_loss, epoch_acc","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:19.975358Z","iopub.execute_input":"2023-12-05T17:45:19.975710Z","iopub.status.idle":"2023-12-05T17:45:19.987053Z","shell.execute_reply.started":"2023-12-05T17:45:19.975680Z","shell.execute_reply":"2023-12-05T17:45:19.986117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation (for a single epoch)","metadata":{}},{"cell_type":"code","source":"@torch.inference_mode()\ndef validation_epoch(model, dataloader, device, epoch):\n    model.eval()\n    \n    data_size = 0\n    loss = 0.0\n    accuracy = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:        \n        images = data['image'].to(device, dtype=torch.float)\n        labels = data['label'].to(device, dtype=torch.long)\n        \n        batch_size = images.size(0)\n\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        _, predicted = torch.max(model.softmax(outputs), 1)\n        acc = torch.sum( predicted == labels )\n\n        loss += (loss.item() * batch_size)\n        accuracy  += acc.item()\n        data_size += batch_size\n        \n        epoch_loss = loss / data_size\n        epoch_acc = accuracy / data_size\n        \n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss, Valid_Acc=epoch_acc,\n                        LR=optimizer.param_groups[0]['lr'])   \n    \n    gc.collect()\n    \n    return epoch_loss, epoch_acc","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:19.988018Z","iopub.execute_input":"2023-12-05T17:45:19.988374Z","iopub.status.idle":"2023-12-05T17:45:20.000824Z","shell.execute_reply.started":"2023-12-05T17:45:19.988349Z","shell.execute_reply":"2023-12-05T17:45:19.999913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the Model (over all epochs)","metadata":{}},{"cell_type":"code","source":"def run_training(model, optimizer, scheduler, device, num_epochs):\n    if torch.cuda.is_available():\n        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_acc = -np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        train_epoch_loss, train_epoch_acc = train_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader, \n                                           device=CONFIG['device'], epoch=epoch)\n        \n        val_epoch_loss, val_epoch_acc = validation_epoch(model, valid_loader, device=CONFIG['device'], \n                                         epoch=epoch)\n    \n        # Record the data in history (which will then be converted into a csv will all results)\n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(val_epoch_loss)\n        history['Train Accuracy'].append(train_epoch_acc)\n        history['Valid Accuracy'].append(val_epoch_acc)\n        history['lr'].append( scheduler.get_lr()[0] )\n        \n        # Copy the current best-performing model\n        if best_epoch_acc <= val_epoch_acc:\n            print(f\"{b_}Validation Accuracy Improved ({best_epoch_acc} ---> {val_epoch_acc})\")\n            best_epoch_acc = val_epoch_acc\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = \"Acc{:.2f}_Loss{:.4f}_epoch{:.0f}.bin\".format(best_epoch_acc, val_epoch_loss, epoch)\n            \n            # Save the current model until a better model is found\n            torch.save(model.state_dict(), PATH)\n    \n    # Display training completion statistics\n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best Accuracy: {:.4f}\".format(best_epoch_acc))\n    \n    # Retrieve the best model and its parameters to be returned\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:20.002111Z","iopub.execute_input":"2023-12-05T17:45:20.002880Z","iopub.status.idle":"2023-12-05T17:45:20.014114Z","shell.execute_reply.started":"2023-12-05T17:45:20.002854Z","shell.execute_reply":"2023-12-05T17:45:20.013183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_scheduler(optimizer):\n    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n                                                   eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n                                                             eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == None:\n        return None\n        \n    return scheduler","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:20.015213Z","iopub.execute_input":"2023-12-05T17:45:20.015489Z","iopub.status.idle":"2023-12-05T17:45:20.026263Z","shell.execute_reply.started":"2023-12-05T17:45:20.015465Z","shell.execute_reply":"2023-12-05T17:45:20.025379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare for Training Execution","metadata":{}},{"cell_type":"markdown","source":"## Prepare Loaders","metadata":{}},{"cell_type":"code","source":"def prepare_loaders(df, fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = UBCData(df_train, transforms=data_transforms[\"train\"])\n    valid_dataset = UBCData(df_valid, transforms=data_transforms[\"valid\"])\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n                              num_workers=2, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:20.027412Z","iopub.execute_input":"2023-12-05T17:45:20.028021Z","iopub.status.idle":"2023-12-05T17:45:20.038881Z","shell.execute_reply.started":"2023-12-05T17:45:20.027988Z","shell.execute_reply":"2023-12-05T17:45:20.038035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader, valid_loader = prepare_loaders(df, fold=CONFIG[\"fold\"])","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:20.039915Z","iopub.execute_input":"2023-12-05T17:45:20.040197Z","iopub.status.idle":"2023-12-05T17:45:20.050828Z","shell.execute_reply.started":"2023-12-05T17:45:20.040174Z","shell.execute_reply":"2023-12-05T17:45:20.049855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initialize Optimizer and Scheduler","metadata":{}},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], \n                       weight_decay=CONFIG['weight_decay'])\nscheduler = fetch_scheduler(optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:20.051972Z","iopub.execute_input":"2023-12-05T17:45:20.052297Z","iopub.status.idle":"2023-12-05T17:45:20.060296Z","shell.execute_reply.started":"2023-12-05T17:45:20.052273Z","shell.execute_reply":"2023-12-05T17:45:20.059381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run Training","metadata":{}},{"cell_type":"code","source":"model, history = run_training(model, optimizer, scheduler,\n                              device=CONFIG['device'],\n                              num_epochs=CONFIG['epochs'])","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:45:20.061287Z","iopub.execute_input":"2023-12-05T17:45:20.061564Z","iopub.status.idle":"2023-12-05T17:57:39.238298Z","shell.execute_reply.started":"2023-12-05T17:45:20.061540Z","shell.execute_reply":"2023-12-05T17:57:39.237202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Write Results to CSV","metadata":{}},{"cell_type":"code","source":"history = pd.DataFrame.from_dict(history)\nhistory.to_csv(\"history.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:57:39.242539Z","iopub.execute_input":"2023-12-05T17:57:39.242897Z","iopub.status.idle":"2023-12-05T17:57:39.251697Z","shell.execute_reply.started":"2023-12-05T17:57:39.242858Z","shell.execute_reply":"2023-12-05T17:57:39.250864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Result Visualization","metadata":{}},{"cell_type":"code","source":"plt.plot( range(history.shape[0]), history[\"Train Loss\"].values, label=\"Train Loss\")\nplt.plot( range(history.shape[0]), history[\"Valid Loss\"].values, label=\"Valid Loss\")\nplt.xlabel(\"# of Epochs\")\nplt.ylabel(\"Loss\")\nplt.grid()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:57:39.252964Z","iopub.execute_input":"2023-12-05T17:57:39.255609Z","iopub.status.idle":"2023-12-05T17:57:39.554066Z","shell.execute_reply.started":"2023-12-05T17:57:39.255571Z","shell.execute_reply":"2023-12-05T17:57:39.553098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot( range(history.shape[0]), history[\"Train Accuracy\"].values, label=\"Train Accuracy\")\nplt.plot( range(history.shape[0]), history[\"Valid Accuracy\"].values, label=\"Valid Accuracy\")\nplt.xlabel(\"# of Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.grid()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:57:39.555342Z","iopub.execute_input":"2023-12-05T17:57:39.555672Z","iopub.status.idle":"2023-12-05T17:57:39.846697Z","shell.execute_reply.started":"2023-12-05T17:57:39.555639Z","shell.execute_reply":"2023-12-05T17:57:39.845803Z"},"trusted":true},"execution_count":null,"outputs":[]}]}